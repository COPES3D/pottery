{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('/data/codes/pottery'))\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'models'))\n",
    "sys.path.append(os.path.join(BASE_DIR, 'utils'))\n",
    "import provider\n",
    "import utils.tf_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--gpu GPU] [--model MODEL]\n",
      "                             [--log_dir LOG_DIR] [--num_point NUM_POINT]\n",
      "                             [--max_epoch MAX_EPOCH] [--batch_size BATCH_SIZE]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--momentum MOMENTUM] [--optimizer OPTIMIZER]\n",
      "                             [--decay_step DECAY_STEP]\n",
      "                             [--decay_rate DECAY_RATE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1003/jupyter/kernel-75f18a42-1c63-489e-8dec-420ab4b62d5e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tf/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--gpu', type=int, default=0, help='GPU to use [default: GPU 0]')\n",
    "# parser.add_argument('--model', default='dgcnn', help='Model name: dgcnn')\n",
    "# parser.add_argument('--log_dir', default='log', help='Log dir [default: log]')\n",
    "# parser.add_argument('--num_point', type=int, default=1024, help='Point Number [256/512/1024/2048] [default: 1024]')\n",
    "# parser.add_argument('--max_epoch', type=int, default=250, help='Epoch to run [default: 250]')\n",
    "# parser.add_argument('--batch_size', type=int, default=32, help='Batch Size during training [default: 32]')\n",
    "# parser.add_argument('--learning_rate', type=float, default=0.001, help='Initial learning rate [default: 0.001]')\n",
    "# parser.add_argument('--momentum', type=float, default=0.9, help='Initial learning rate [default: 0.9]')\n",
    "# parser.add_argument('--optimizer', default='adam', help='adam or momentum [default: adam]')\n",
    "# parser.add_argument('--decay_step', type=int, default=200000, help='Decay step for lr decay [default: 200000]')\n",
    "# parser.add_argument('--decay_rate', type=float, default=0.7, help='Decay rate for lr decay [default: 0.8]')\n",
    "# FLAGS = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "FLAGS=easydict.EasyDict({\n",
    "    \"gpu\" : 0,\n",
    "    \"model\" : 'dgcnn',\n",
    "    \"log_dir\":  'log',\n",
    "    \"num_point\": 1024,\n",
    "    'max_epoch': 250,\n",
    "    'batch_size':32,\n",
    "    'learning_rate':0.001,\n",
    "    'momentum':0.9,\n",
    "    'optimizer': 'adam',\n",
    "    'decay_step': 200000,\n",
    "    'decay_rate': 0.7\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = FLAGS.batch_size\n",
    "NUM_POINT = FLAGS.num_point\n",
    "MAX_EPOCH = FLAGS.max_epoch\n",
    "BASE_LEARNING_RATE = FLAGS.learning_rate\n",
    "GPU_INDEX = FLAGS.gpu\n",
    "MOMENTUM = FLAGS.momentum\n",
    "OPTIMIZER = FLAGS.optimizer\n",
    "DECAY_STEP = FLAGS.decay_step\n",
    "DECAY_RATE = FLAGS.decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'transform_nets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4c62cf8d725a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# import network module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/tf/lib/python3.5/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m/data/codes/pottery/dgcnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../../utils'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransform_nets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_transform_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'transform_nets'"
     ]
    }
   ],
   "source": [
    "MODEL = importlib.import_module(FLAGS.model) # import network module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_FILE = os.path.join(BASE_DIR, 'models', FLAGS.model+'.py')\n",
    "LOG_DIR = FLAGS.log_dir\n",
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "os.system('cp %s %s' % (MODEL_FILE, LOG_DIR)) # bkp of model def\n",
    "os.system('cp train.py %s' % (LOG_DIR)) # bkp of train procedure\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "LOG_FOUT.write(str(FLAGS)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_POINT = 2048\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOSTNAME = socket.gethostname()\n",
    "\n",
    "# ModelNet40 official train/test split\n",
    "TRAIN_FILES = provider.getDataFiles( \\\n",
    "    os.path.join('./data/modelnet40_ply_hdf5_2048/train_files.txt'))\n",
    "TEST_FILES = provider.getDataFiles(\\\n",
    "    os.path.join('./data/modelnet40_ply_hdf5_2048/test_files.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                        DECAY_STEP,          # Decay step.\n",
    "                        DECAY_RATE,          # Decay rate.\n",
    "                        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "    return learning_rate        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "                      BN_INIT_DECAY,\n",
    "                      batch*BATCH_SIZE,\n",
    "                      BN_DECAY_DECAY_STEP,\n",
    "                      BN_DECAY_DECAY_RATE,\n",
    "                      staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            print(is_training_pl)\n",
    "            \n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "\n",
    "            # Get model and loss \n",
    "            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = MODEL.get_loss(pred, labels_pl, end_points)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "\n",
    "            correct = tf.equal(tf.argmax(pred, 1), tf.to_int64(labels_pl))\n",
    "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            # Get training operator\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "            \n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "        # Create a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        # Add summary writers\n",
    "        #merged = tf.merge_all_summaries()\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'),\n",
    "                                  sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
    "\n",
    "        # Init variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        # To fix the bug introduced in TF 0.12.1 as in\n",
    "        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
    "        #sess.run(init)\n",
    "        sess.run(init, {is_training_pl: True})\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'labels_pl': labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch}\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "            log_string('**** EPOCH %03d ****' % (epoch))\n",
    "            sys.stdout.flush()\n",
    "             \n",
    "            train_one_epoch(sess, ops, train_writer)\n",
    "            eval_one_epoch(sess, ops, test_writer)\n",
    "            \n",
    "            # Save the variables to disk.\n",
    "            if epoch % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"))\n",
    "                log_string(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(sess, ops, train_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "    \n",
    "    # Shuffle train files\n",
    "    train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "    np.random.shuffle(train_file_idxs)\n",
    "    \n",
    "    for fn in range(len(TRAIN_FILES)):\n",
    "        log_string('----' + str(fn) + '-----')\n",
    "        current_data, current_label = provider.loadDataFile(TRAIN_FILES[train_file_idxs[fn]])\n",
    "        current_data = current_data[:,0:NUM_POINT,:]\n",
    "        current_data, current_label, _ = provider.shuffle_data(current_data, np.squeeze(current_label))            \n",
    "        current_label = np.squeeze(current_label)\n",
    "        \n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "        \n",
    "        total_correct = 0\n",
    "        total_seen = 0\n",
    "        loss_sum = 0\n",
    "       \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "            \n",
    "            # Augment batched point clouds by rotation and jittering\n",
    "            rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
    "            jittered_data = provider.jitter_point_cloud(rotated_data)\n",
    "            jittered_data = provider.random_scale_point_cloud(jittered_data)\n",
    "            jittered_data = provider.rotate_perturbation_point_cloud(jittered_data)\n",
    "            jittered_data = provider.shift_point_cloud(jittered_data)\n",
    "\n",
    "            feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
    "                         ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "                         ops['is_training_pl']: is_training,}\n",
    "            summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "            train_writer.add_summary(summary, step)\n",
    "            pred_val = np.argmax(pred_val, 1)\n",
    "            correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "            total_correct += correct\n",
    "            total_seen += BATCH_SIZE\n",
    "            loss_sum += loss_val\n",
    "        \n",
    "        log_string('mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "        log_string('accuracy: %f' % (total_correct / float(total_seen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def eval_one_epoch(sess, ops, test_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "    for fn in range(len(TEST_FILES)):\n",
    "        log_string('----' + str(fn) + '-----')\n",
    "        current_data, current_label = provider.loadDataFile(TEST_FILES[fn])\n",
    "        current_data = current_data[:,0:NUM_POINT,:]\n",
    "        current_label = np.squeeze(current_label)\n",
    "        \n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "            feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                         ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "                         ops['is_training_pl']: is_training}\n",
    "            summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "            pred_val = np.argmax(pred_val, 1)\n",
    "            correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "            total_correct += correct\n",
    "            total_seen += BATCH_SIZE\n",
    "            loss_sum += (loss_val*BATCH_SIZE)\n",
    "            for i in range(start_idx, end_idx):\n",
    "                l = current_label[i]\n",
    "                total_seen_class[l] += 1\n",
    "                total_correct_class[l] += (pred_val[i-start_idx] == l)\n",
    "            \n",
    "    log_string('eval mean loss: %f' % (loss_sum / float(total_seen)))\n",
    "    log_string('eval accuracy: %f'% (total_correct / float(total_seen)))\n",
    "    log_string('eval avg class acc: %f' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--gpu GPU] [--model MODEL]\n",
      "                             [--log_dir LOG_DIR] [--num_point NUM_POINT]\n",
      "                             [--max_epoch MAX_EPOCH] [--batch_size BATCH_SIZE]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--momentum MOMENTUM] [--optimizer OPTIMIZER]\n",
      "                             [--decay_step DECAY_STEP]\n",
      "                             [--decay_rate DECAY_RATE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1003/jupyter/kernel-75f18a42-1c63-489e-8dec-420ab4b62d5e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tf/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    LOG_FOUT.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
