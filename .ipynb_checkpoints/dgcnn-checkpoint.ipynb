{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, '../utils'))\n",
    "sys.path.append(os.path.join(BASE_DIR, '../../utils'))\n",
    "import tf_util\n",
    "from transform_nets import input_transform_net\n",
    "\n",
    "\n",
    "def placeholder_inputs(batch_size, num_point):\n",
    "  pointclouds_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, 3))\n",
    "  labels_pl = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "  return pointclouds_pl, labels_pl\n",
    "\n",
    "\n",
    "def get_model(point_cloud, is_training, bn_decay=None):\n",
    "  \"\"\" Classification PointNet, input is BxNx3, output Bx40 \"\"\"\n",
    "  batch_size = point_cloud.get_shape()[0].value\n",
    "  num_point = point_cloud.get_shape()[1].value\n",
    "  end_points = {}\n",
    "  k = 20\n",
    "\n",
    "  adj_matrix = tf_util.pairwise_distance(point_cloud)\n",
    "  nn_idx = tf_util.knn(adj_matrix, k=k)\n",
    "  edge_feature = tf_util.get_edge_feature(point_cloud, nn_idx=nn_idx, k=k)\n",
    "\n",
    "  with tf.variable_scope('transform_net1') as sc:\n",
    "    transform = input_transform_net(edge_feature, is_training, bn_decay, K=3)\n",
    "\n",
    "  point_cloud_transformed = tf.matmul(point_cloud, transform)\n",
    "  adj_matrix = tf_util.pairwise_distance(point_cloud_transformed)\n",
    "  nn_idx = tf_util.knn(adj_matrix, k=k)\n",
    "  edge_feature = tf_util.get_edge_feature(point_cloud_transformed, nn_idx=nn_idx, k=k)\n",
    "\n",
    "  net = tf_util.conv2d(edge_feature, 64, [1,1],\n",
    "                       padding='VALID', stride=[1,1],\n",
    "                       bn=True, is_training=is_training,\n",
    "                       scope='dgcnn1', bn_decay=bn_decay)\n",
    "  net = tf.reduce_max(net, axis=-2, keep_dims=True)\n",
    "  net1 = net\n",
    "\n",
    "  adj_matrix = tf_util.pairwise_distance(net)\n",
    "  nn_idx = tf_util.knn(adj_matrix, k=k)\n",
    "  edge_feature = tf_util.get_edge_feature(net, nn_idx=nn_idx, k=k)\n",
    "\n",
    "  net = tf_util.conv2d(edge_feature, 64, [1,1],\n",
    "                       padding='VALID', stride=[1,1],\n",
    "                       bn=True, is_training=is_training,\n",
    "                       scope='dgcnn2', bn_decay=bn_decay)\n",
    "  net = tf.reduce_max(net, axis=-2, keep_dims=True)\n",
    "  net2 = net\n",
    " \n",
    "  adj_matrix = tf_util.pairwise_distance(net)\n",
    "  nn_idx = tf_util.knn(adj_matrix, k=k)\n",
    "  edge_feature = tf_util.get_edge_feature(net, nn_idx=nn_idx, k=k)  \n",
    "\n",
    "  net = tf_util.conv2d(edge_feature, 64, [1,1],\n",
    "                       padding='VALID', stride=[1,1],\n",
    "                       bn=True, is_training=is_training,\n",
    "                       scope='dgcnn3', bn_decay=bn_decay)\n",
    "  net = tf.reduce_max(net, axis=-2, keep_dims=True)\n",
    "  net3 = net\n",
    "\n",
    "  adj_matrix = tf_util.pairwise_distance(net)\n",
    "  nn_idx = tf_util.knn(adj_matrix, k=k)\n",
    "  edge_feature = tf_util.get_edge_feature(net, nn_idx=nn_idx, k=k)  \n",
    "  \n",
    "  net = tf_util.conv2d(edge_feature, 128, [1,1],\n",
    "                       padding='VALID', stride=[1,1],\n",
    "                       bn=True, is_training=is_training,\n",
    "                       scope='dgcnn4', bn_decay=bn_decay)\n",
    "  net = tf.reduce_max(net, axis=-2, keep_dims=True)\n",
    "  net4 = net\n",
    "\n",
    "  net = tf_util.conv2d(tf.concat([net1, net2, net3, net4], axis=-1), 1024, [1, 1], \n",
    "                       padding='VALID', stride=[1,1],\n",
    "                       bn=True, is_training=is_training,\n",
    "                       scope='agg', bn_decay=bn_decay)\n",
    " \n",
    "  net = tf.reduce_max(net, axis=1, keep_dims=True) \n",
    "\n",
    "  # MLP on global point cloud vector\n",
    "  net = tf.reshape(net, [batch_size, -1]) \n",
    "  net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training,\n",
    "                                scope='fc1', bn_decay=bn_decay)\n",
    "  net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training,\n",
    "                         scope='dp1')\n",
    "  net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training,\n",
    "                                scope='fc2', bn_decay=bn_decay)\n",
    "  net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training,\n",
    "                        scope='dp2')\n",
    "  net = tf_util.fully_connected(net, 40, activation_fn=None, scope='fc3')\n",
    "\n",
    "  return net, end_points\n",
    "\n",
    "\n",
    "def get_loss(pred, label, end_points):\n",
    "  \"\"\" pred: B*NUM_CLASSES,\n",
    "      label: B, \"\"\"\n",
    "  labels = tf.one_hot(indices=label, depth=40)\n",
    "  loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=pred, label_smoothing=0.2)\n",
    "  classify_loss = tf.reduce_mean(loss)\n",
    "  return classify_loss\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "  batch_size = 2\n",
    "  num_pt = 124\n",
    "  pos_dim = 3\n",
    "\n",
    "  input_feed = np.random.rand(batch_size, num_pt, pos_dim)\n",
    "  label_feed = np.random.rand(batch_size)\n",
    "  label_feed[label_feed>=0.5] = 1\n",
    "  label_feed[label_feed<0.5] = 0\n",
    "  label_feed = label_feed.astype(np.int32)\n",
    "\n",
    "  # # np.save('./debug/input_feed.npy', input_feed)\n",
    "  # input_feed = np.load('./debug/input_feed.npy')\n",
    "  # print input_feed\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "    input_pl, label_pl = placeholder_inputs(batch_size, num_pt)\n",
    "    pos, ftr = get_model(input_pl, tf.constant(True))\n",
    "    # loss = get_loss(logits, label_pl, None)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "      sess.run(tf.global_variables_initializer())\n",
    "      feed_dict = {input_pl: input_feed, label_pl: label_feed}\n",
    "      res1, res2 = sess.run([pos, ftr], feed_dict=feed_dict)\n",
    "      print (res1.shape)\n",
    "      print (res1)\n",
    "\n",
    "      print (res2.shape)\n",
    "      print (res2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
